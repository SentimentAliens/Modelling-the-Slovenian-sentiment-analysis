# Procedure for detecting possible the erronous labels in dataset


# 1. Using existing model without training multiple model 
This is a great task that directly applies the power of **cleanlab** to a domain-specific model and dataset. Cleanlab works by using a model's predicted probabilities to identify examples where the observed label is likely inconsistent with the predicted label.

The most critical step is generating **out-of-sample predicted probabilities** ($\hat{P}$) for every instance in your dataset. This requires **K-Fold Cross-Validation (CV)**.

Here is the step-by-step guide on how to proceed with the `classla/xlm-r-parlasent` model and the KKS 1.001 corpus.

-----

## Prerequisites: Install Libraries

You will need to install `cleanlab`, the Hugging Face `transformers` and `datasets` libraries, and `scikit-learn`.

```bash
# Install the core libraries
pip install cleanlab datasets transformers torch scikit-learn numpy pandas
```

-----

## Step 1: Data Preparation and Loading

First, you must load the KKS 1.001 dataset and map its labels to numerical integers. You must download the dataset from the CLARIN link provided, as it is not directly available on Hugging Face.

**Assumptions:**

1.  You have loaded the KKS 1.001 corpus into a **Pandas DataFrame** named `df`.
2.  The text content is in a column named **`text`**.
3.  The true sentiment labels are in a column named **`label`** (e.g., 'Negative', 'Neutral', 'Positive').

<!-- end list -->

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import KFold

# --- Example of creating a dummy DataFrame (REPLACE THIS WITH YOUR ACTUAL DATA LOADING) ---
# df = pd.read_csv('kks_1001_corpus.csv', sep='\t')
# df = df[['text_column', 'label_column']].rename(columns={'text_column': 'text', 'label_column': 'label'})
# ---------------------------------------------------------------------------------------

# 1. Map string labels to integer classes (Required for cleanlab and model output)
label_mapping = {'Negative': 0, 'Neutral': 1, 'Positive': 2}
df['label_int'] = df['label'].map(label_mapping)

# Extract inputs
data_texts = df['text'].tolist()
true_labels = df['label_int'].values # This is Y, the noisy labels
NUM_CLASSES = len(label_mapping)

print(f"Data loaded with {len(df)} examples and {NUM_CLASSES} classes.")
```

-----

## Step 2: Model and Tokenizer Setup

Load the necessary components from Hugging Face for the `classla/xlm-r-parlasent` model. We load it using `AutoModelForSequenceClassification` and specify the number of labels to ensure it outputs logits for our 3 classes (Negative, Neutral, Positive).

```python
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

MODEL_NAME = "classla/xlm-r-parlasent"

# Load Tokenizer
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# Load Model: Force it into 3-class classification mode, assuming we are interested
# in the core Negative/Neutral/Positive classification, not the 6-class regression.
model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=NUM_CLASSES # Set to 3
)
model.eval() # Set the model to evaluation mode
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
```

-----

## Step 3: Generate Out-of-Sample Probabilities ($\hat{P}$) using K-Fold CV

**Cleanlab** requires the predicted probabilities for each example to be generated by a model that **was NOT trained on that specific example**. This is achieved by running K-Fold Cross-Validation (CV).

1.  The dataset is split into $K$ folds (e.g., $K=5$).
2.  For each fold, a model is **trained** on the other $K-1$ folds (the training set).
3.  The model then generates predictions for the held-out fold (the test set).
4.  The predictions from all $K$ folds are combined to get the predicted probabilities $\hat{P}$ for the entire dataset.

Since **re-training the model is computationally expensive**, we will use a practical simplification for this guide: we will use the **pre-trained** `classla/xlm-r-parlasent` model to predict probabilities for the entire dataset.

**NOTE:** *For the most robust and accurate **cleanlab** results, you would typically need to perform the full CV process where you re-train the model in each fold. The method below uses the model's zero-shot performance on the dataset, which is a sufficient starting point.*

```python
from torch.nn.functional import softmax

BATCH_SIZE = 32
predicted_probabilities = np.zeros((len(data_texts), NUM_CLASSES))

print(f"Generating probabilities using the pre-trained model on {len(data_texts)} examples...")

# Helper function to process batches
def get_probabilities(texts):
    inputs = tokenizer(texts, return_tensors="pt", padding=True, truncation=True).to(device)
    with torch.no_grad():
        outputs = model(**inputs)
    
    # Get logits and convert to probabilities using Softmax
    logits = outputs.logits
    probs = softmax(logits, dim=1).cpu().numpy()
    return probs

# Process data in batches
for i in range(0, len(data_texts), BATCH_SIZE):
    batch_texts = data_texts[i:i + BATCH_SIZE]
    batch_probs = get_probabilities(batch_texts)
    predicted_probabilities[i:i + BATCH_SIZE] = batch_probs

print("Finished generating probabilities.")
```

-----

## Step 4: Run Cleanlab to Find Label Issues

With the true labels ($Y$) and the out-of-sample predicted probabilities ($\hat{P}$) ready, we can now use `cleanlab`.

```python
from cleanlab.filter import find_label_issues

# Find potential label issues
# Y = true_labels (numerical array)
# P = predicted_probabilities (numpy array of shape [n_samples, n_classes])
ranked_label_issues = find_label_issues(
    labels=true_labels,
    pred_probs=predicted_probabilities,
    return_indices_ranked=True # Returns indices ranked by likelihood of being a label error
)

# Convert the list of issue indices to a set for quick lookup
issue_indices = set(ranked_label_issues)

# Add the issue scores and original data to the DataFrame
df['is_label_issue'] = df.index.isin(issue_indices)
df['label_quality_score'] = find_label_issues(
    labels=true_labels,
    pred_probs=predicted_probabilities,
    return_label_quality_scores=True
)

# Sort by the score (lowest score is the most likely error)
df_issues = df.sort_values(by='label_quality_score').head(20) # Show top 20 potential issues
```

-----

## Step 5: Review and Validate Results

The `df_issues` DataFrame now contains the examples most likely to have incorrect labels, ranked by the `label_quality_score` (lower score = higher probability of error).

You should manually review the top entries:

```python
# Display the top 5 most likely label errors
print("\n--- Top 5 Most Likely Label Errors ---")
for index, row in df_issues.head(5).iterrows():
    print(f"\n--- Index: {index} (Score: {row['label_quality_score']:.4f}) ---")
    print(f"Original Label: {row['label']}")
    print(f"Comment Text: \"{row['text'][:100]}...\"") # Print first 100 characters

# To understand why the model thinks the label is wrong,
# compare the true label index to the predicted probability array:
# E.g., for the first issue:
first_issue_index = df_issues.index[0]
probs = predicted_probabilities[first_issue_index]
print("\nPredicted Probabilities (Negative, Neutral, Positive):")
print(probs)
```

**Interpretation:**

  * **Original Label:** The label currently assigned in the KKS 1.001 dataset.
  * **Predicted Probabilities:** The probability the model assigns to each class.
  * **Label Quality Score:** A value calculated by cleanlab that measures how inconsistent the given label is with the predicted probabilities. A score close to $\mathbf{0}$ means the given label is highly suspicious.

If you see an example originally labeled **'Positive'**, but the model assigns probabilities like `[0.9, 0.05, 0.05]` (90% Negative, 5% Neutral, 5% Positive), the label quality score will be low, indicating that the original label ('Positive') is almost certainly an error.

# 2. K-fold validation approach of training multiple times.
That's an excellent approach for using **cleanlab** robustly\! Generating out-of-sample predicted probabilities ($\hat{P}$) via K-Fold Cross-Validation (CV) is the gold standard.

Since training a large transformer model like `classla/xlm-r-parlasent` across multiple folds is computationally intensive, the steps below use the popular **Hugging Face Trainer API** within a K-Fold loop to manage the training and prediction process efficiently.

Here is the step-by-step guide for implementing the full K-Fold CV approach.

-----

## üõ†Ô∏è Step 1: Data Preparation and Splitting

We'll start by converting your DataFrame into a Hugging Face `Dataset` object, which works seamlessly with the `transformers` library and `cleanlab`.

```python
import pandas as pd
import numpy as np
from datasets import Dataset
from sklearn.model_selection import KFold

# --- Assume you have your pre-processed DataFrame 'df' from the previous steps ---
# df = pd.read_csv('kks_1001_corpus.csv')
# df['label_int'] = df['label'].map({'Negative': 0, 'Neutral': 1, 'Positive': 2})
# -----------------------------------------------------------------------------------

# Rename the label column for Trainer compatibility
df_hf = df.rename(columns={'label_int': 'labels'})

# Create a full Hugging Face Dataset from the DataFrame
full_dataset = Dataset.from_pandas(df_hf)

# Initialize K-Fold Splitter
K = 5 # Standard number of folds
kf = KFold(n_splits=K, shuffle=True, random_state=42)

# Initialize array to store out-of-sample probabilities
# Shape: [Total number of examples, Number of classes (3)]
true_labels = full_dataset['labels']
NUM_CLASSES = len(np.unique(true_labels))
OOS_pred_probs = np.zeros((len(full_dataset), NUM_CLASSES))

print(f"Prepared dataset with {len(full_dataset)} examples for {K}-Fold Cross-Validation.")
```

-----

## ‚öôÔ∏è Step 2: Model and Trainer Setup

We'll load the model and tokenizer, and define the training arguments once.

```python
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer

MODEL_NAME = "classla/xlm-r-parlasent"
OUTPUT_DIR = "./kfold_results" # Temporary directory for model checkpoints

# Load Tokenizer
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# Function to tokenize the data
def tokenize_function(examples):
    # Truncate to max length for the model, use text content from the column name
    return tokenizer(examples['text'], padding="max_length", truncation=True)

# Apply tokenization to the full dataset (saves time outside the loop)
tokenized_dataset = full_dataset.map(tokenize_function, batched=True)

# Define Training Arguments (adjust learning rate, epochs, and batch size based on resources)
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    num_train_epochs=3,                   # Fine-tuning for 3 epochs
    per_device_train_batch_size=16,       # Adjust based on GPU VRAM
    per_device_eval_batch_size=32,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=100,
    save_strategy="epoch",                # Save model at the end of each epoch
    load_best_model_at_end=False,         # Not strictly needed for CV OOS probs
    report_to="none"                      # Disable external logging for simplicity
)

print("Tokenizer and Training Arguments defined.")
```

-----

## üîÅ Step 3: Run K-Fold Cross-Validation

This is the core step where the model is re-initialized and trained $K$ times, generating out-of-sample predictions for each fold.

```python
print("\n--- Starting K-Fold Cross-Validation ---")

for fold_idx, (train_index, test_index) in enumerate(kf.split(tokenized_dataset)):
    print(f"\nProcessing Fold {fold_idx + 1}/{K}...")
    
    # 1. Create train and test subsets for the current fold
    train_dataset = tokenized_dataset.select(train_index)
    test_dataset = tokenized_dataset.select(test_index)
    
    # 2. Re-initialize the model for the new fold (crucial to start training fresh)
    model = AutoModelForSequenceClassification.from_pretrained(
        MODEL_NAME, 
        num_labels=NUM_CLASSES
    )
    
    # 3. Initialize Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
    )
    
    # 4. Train the model on the K-1 folds (train_dataset)
    trainer.train()
    
    # 5. Get predictions on the held-out fold (test_dataset)
    predictions = trainer.predict(test_dataset)
    
    # 6. Store the out-of-sample predicted probabilities (logits converted to probs)
    probs = torch.nn.functional.softmax(
        torch.from_numpy(predictions.predictions), dim=1
    ).numpy()
    
    # Place probabilities into the main OOS array at the correct test indices
    OOS_pred_probs[test_index] = probs
    
    # Optional: Delete model and clear cache to free up VRAM for the next fold
    del model
    del trainer
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

print("\n--- K-Fold CV Complete. OOS Probabilities Generated. ---")
```

-----

## ‚úÖ Step 4: Run Cleanlab

Now that you have the complete array of **out-of-sample predicted probabilities** ($\hat{P}$ = `OOS_pred_probs`), you can run `cleanlab.filter.find_label_issues`.

```python
from cleanlab.filter import find_label_issues

# Find potential label issues
# labels = true_labels (Y)
# pred_probs = OOS_pred_probs (P_hat)
ranked_label_issues = find_label_issues(
    labels=true_labels,
    pred_probs=OOS_pred_probs,
    return_indices_ranked=True
)

# Generate quality scores
label_quality_scores = find_label_issues(
    labels=true_labels,
    pred_probs=OOS_pred_probs,
    return_label_quality_scores=True
)

# Map results back to the original DataFrame
df['is_label_issue'] = df.index.isin(set(ranked_label_issues))
df['label_quality_score'] = label_quality_scores

# Sort and display the top 10 most likely noisy labels
df_issues = df.sort_values(by='label_quality_score').head(10)

print("\n--- Top 10 Potential Label Issues Found by Cleanlab ---")
print(df_issues[['text', 'label', 'label_quality_score']])

# The indices in `ranked_label_issues` correspond to the rows in your original DataFrame
print(f"\nTotal number of potential label issues identified: {len(ranked_label_issues)}")
```

This procedure ensures that every prediction used by cleanlab is **unbiased** by training on its own label, giving you the most reliable list of potentially erroneous labels in your KKS 1.001 corpus.